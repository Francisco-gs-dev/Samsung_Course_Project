{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacemos la llamada a la API para obtener datos del clima desde el año 2014 hasta el actual (2024)\n",
    "\n",
    "- En primer lugar creamos una función (fetch_data) para consumir la api, esta función necesita de tres parámetros: **start_str(fecha de inicio), end_str(fecha final) y token**, la clave api key con la que accedemos. Esta petición nos devuelve el siguiente formato:\n",
    "\n",
    "        b'{\\n  \"descripcion\" : \"exito\",\\n  \"estado\" : 200,\\n  \"datos\" : \"https://opendata.aemet.es/opendata/sh/161db1f3\",\\n  \"metadatos\" : \"https://opendata.aemet.es/opendata/sh/b3aa9d28\"\\n}'\n",
    "\n",
    "\n",
    "- Devolvemos el valor de la clave **\"datos\"**, ya que en esa url se encuentran los datos que queremos obtener en formato Json\n",
    "\n",
    "- Luego en la función fetch_and_convert_data accedemos a la esta url, obtenemos los datos en formato json y los normalizamos para guardarlos en un dataframe de pandas\n",
    "\n",
    "- Por último tenemos la función fetch_and_combine_data, a la que le indicamos en sus parámetros la fecha de comienzo y de final total para la extracción de los datos. Para ello hemos accedido por años, ya que la API bloquea las llamadas cada cierto tiempo/volumen de llamadas. Dentro de la función se divide el año en lotes de 15 días iguales, y se llama a las dos funciones anteriores para obtener los csv correspondientes a cada fracción del tiempo que le pasemos. Itera hasta terminar el año, y mientras va iterando va guardando y uniendo los csv en una variable local llamada **all_data_frames**, a la que se llama en último lugar porque es la que contiene el csv anual total. Una vez termina de iterar sobre las fechas correspondientes **se guarda el archivo en formato .csv con el año correspondiente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Función para obtener las URLs desde la API de la AEMET\n",
    "def fetch_data(start_date, end_date):\n",
    "    conn = http.client.HTTPSConnection(\"opendata.aemet.es\")\n",
    "    \n",
    "    # Formatear las fechas a formato adecuado para la URL\n",
    "    start_str = start_date.strftime(\"%Y-%m-%dT%H%%3A%M%%3A%SUTC\")\n",
    "    end_str = end_date.strftime(\"%Y-%m-%dT%H%%3A%M%%3A%SUTC\")\n",
    "    \n",
    "    headers = {\n",
    "        'cache-control': \"no-cache\"\n",
    "    }\n",
    "    \n",
    "    token = \"\"  # Aquí hay que indicarle el token de acceso que se obtiene en el siguiente enlace https://opendata.aemet.es/centrodedescargas/altaUsuario?\n",
    "    \n",
    "    # Realizar la solicitud para obtener las URLs de los datos\n",
    "    conn.request(\"GET\", f\"/opendata/api/valores/climatologicos/diarios/datos/fechaini/{start_str}/fechafin/{end_str}/todasestaciones/?api_key={token}\", headers=headers)\n",
    "    \n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    \n",
    "    # Decodificar la respuesta\n",
    "    json_response = eval(data.decode(\"utf-8\"))\n",
    "    \n",
    "    # Extraer la URL de los datos\n",
    "    data_url = json_response[\"datos\"]\n",
    "    return data_url\n",
    "\n",
    "# Función para descargar los datos desde la URL proporcionada\n",
    "def fetch_and_convert_data(data_url):\n",
    "    response = requests.get(data_url)\n",
    "    \n",
    "    # Si la solicitud fue exitosa (status 200)\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()  # Convertir la respuesta en JSON\n",
    "        df = pd.json_normalize(json_data)  # Convertir los datos en un DataFrame\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Error al obtener los datos de {data_url}\")\n",
    "        return None\n",
    "\n",
    "# Función para dividir el rango de fechas en bloques de 15 días y obtener las URLs automáticamente\n",
    "def fetch_and_combine_data(start_date, end_date):\n",
    "    all_data_frames = []  # Lista para almacenar los DataFrames\n",
    "\n",
    "    current_start = start_date\n",
    "    while current_start < end_date:\n",
    "        current_end = current_start + datetime.timedelta(days=14)  # Bloque de 15 días\n",
    "        if current_end > end_date:\n",
    "            current_end = end_date\n",
    "        \n",
    "        print(f\"Obteniendo datos desde {current_start} hasta {current_end}\")\n",
    "        \n",
    "        # Obtener la URL de los datos para el bloque actual\n",
    "        data_url = fetch_data(current_start, current_end)\n",
    "        \n",
    "        # Descargar y convertir los datos de la URL\n",
    "        df = fetch_and_convert_data(data_url)\n",
    "        if df is not None:\n",
    "            all_data_frames.append(df)  # Añadir el DataFrame a la lista\n",
    "\n",
    "        # Esperar 6 segundos entre solicitudes para evitar el bloqueo\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Actualizar la fecha de inicio para el siguiente bloque\n",
    "        current_start = current_end + datetime.timedelta(days=1)\n",
    "\n",
    "    # Concatenar todos los DataFrames en uno solo\n",
    "    if all_data_frames:\n",
    "        full_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        \n",
    "        # Guardar el DataFrame como archivo CSV\n",
    "        full_df.to_csv(f\"DatosClimatologicos_{start_date.year}.csv\", index=False)\n",
    "        print(f\"Archivo 'DatosClimatologicos_{start_date.year}.csv' guardado con éxito\")\n",
    "    else:\n",
    "        print(\"No se han podido obtener los datos\")\n",
    "\n",
    "# Fechas de inicio y fin del rango\n",
    "start_date = datetime.datetime(2023, 1, 1)\n",
    "end_date = datetime.datetime(2023, 12, 31)\n",
    "\n",
    "# Obtener los datos en intervalos de 15 días y guardarlos en un CSV\n",
    "fetch_and_combine_data(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
